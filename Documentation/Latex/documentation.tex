\documentclass{report}

\input{commands}

%\lhead{Left Header}
%\rhead{Right Header}

\begin{document}

% Change Font and Spacing
\large % change the font size to 12pt
\linespread{1.1} % change the line spacing

% Set the counter
% \setcounter{section}{0}

\chapter{Examples}

Examples can be found in the \texttt{\$PATH/CART-Project/Code/Examples} directory.  Examples explore code generated along the way to creating our program for analyzing the long term performance data of companies.  These examples mostly explore the Gnu Scientific Library (GSL).  Here is a short description of each.
\begin{itemize}

\item Basic Examples

\item Matrix Examples

\item Sampling Examples

\item R Examples
\begin{itemize}
\item \texttt{mvn\_scale\_mixture.R} - This is a prelude to the IID Noise with Gaussian Process Prior model.  We take a multivariate normal with known mean and unknown covariance.  The covariance matrix is of the form $V/\phi$ where $V$ is known and $\phi$ is unknown. The prior distribution for $\phi$ is taken to be Gamma, which is conjugate to the normal distribution in this setting.  The posterior Gamma distribution is calculated.  Includes examples of \emph{sampling from multivariate normal}, \emph{multiple plots}, and \emph{writing plots to file}.
\end{itemize}

\end{itemize}

\chapter{Normal Random Variables}

Normal random variables have a surfeit of properties that make them fundamental probabilistic objects.  One can think of normal random variables as the natural basis of a probability space.  Normal random varialbes are 'normal' because they arise as the target of limit theorems in probabibility, in the sense that when one sums and scales a sequence of random variables, the limit converges to a normal.

Let $(L^2(\Omega), \mcF, \bbP)$ be some abstract probability space.  Notice that we have chosen to work with a Hilbert space.  This is an important component of our analysis and will provide a geometric interpretation.

Following convention we let $\mcN(\mu, \sigma^2)$ be the space of normal random variables with mean $\mu$ and variance $\sigma^2$.  Define the space of all normal random variables as
\[
\mcN = \{ X \in \mcN(\mu, \sigma^2) | \mu \in R, \sigma^2 \geq 0 \}.
\]
First, $N$ is a vector space.  Any linear combination of elements of $N$ remains in $N$.  (We can start to construct a basis for $L^2$ from elements in $N$, however, I'm not sure if this will exhaust the elements of $L^2$.  I think this must go back to the Malliavin calculus, which is a ``Gaussian calculus'' and I think may relate to some sort of similar construction.)  Once consequence of this is given a collection of random variables $X_i \in \mcN$ for $i = 1, \ldots, n$ and another random variable $Y \in N$ we have that the conditional expectation of $Y$ given $\{X_i\}$ is the same as the linear conditional expectation of $Y$ given $\{X_i\}$.  This is the same as saying that the conditional expectation of $Y$ given $\{X_i\}$ is identical to the projection of $Y$ onto the span of $\{X_i\}$.  In particular we can project $Y$ onto $\{X_i\}$ to get
\[
Y = \beta \cdot X + R
\]
where
\[
\beta_i = (Y,X) \; \textmd{ and } \; R = Y - \beta \cdot X.
\]
But since $N$ is a vector space we know that $R$ resides in $N$ as well.  Hence $R$ is normal.  Furthermore, we know that $R$ is perpendicular to any element in the span of $\{X_i\}$.  Hence
\[
\bbE[Y | X ] = \bbE[\beta \cdot X + R \; | X] = \beta \cdot X + 0.
\]
We may thus conclude that the projection of $Y$ onto $\{X_i\}$ coincides with the conditional expectation of $Y$ given $\{X_i\}$.
(Another aside: the \sigalg generated by $\sigma(X)$ is the same as the \sigalg generated by $\sigma(\{X_i\})$.  This is because the cylinder sets of $X$ will correspond to the cylinder sets of $\{X_i\}$.)

(There is a duality between conditional expectation and $L^2$ minimization.  In particular, we can define the conditional expectation of a random variable as
\[
\bbE[Y | X] = {\arg \min}_{Z \in \sigma(X)} \bbE[ (Y - Z)^2 ].
\]
One can derive the standard definition of conditional expectation from the above equality using the calculus of variations.  This harks to our justification of the notion of ``the mean.'' One justification is that the estimate that minimizes the mean square error from the quanitity under question is the mean.  I suppose it is somewhat tautilogical to refer to the mean and then mean square error.  If we were working with scalar quantities then we could look at this we could perhaps look at minimizing the total error (or distance).  I suppose that one can justify the mean along other avenues.  Though I'm not sure what those are.)

\chapter{Models}

\section{IID Noise with a Gaussian Process Prior}

This is a toy model used to familiarize myself with generating synthetic data and Bayesian data analysis.  From a time series perspective one has a Gaussian process $(f_t)$ evolving in time.  Were we to observe the Guassian process on the grid $(t_i)_{i=1}^n$ the joint density would have the general form
\[
f(t_1, \ldots, t_n) \sim \mcN(\mu, \sigma^2 K)
\]
where $\mu$ is a vector of means and $K$ is a covariance matrix.  The specific form of $\mu$ and $K$ depend upon the dynamics we specify for $f$.  Let us assume that we actually take noisy measurements $y_{t_i}$ at times $(t_i)$ and that the perturbations of $f$ are independent and identically distributed normal random variables $(\ep_{t_i})$ with mean zero and covariance $\sigma^2$.  The observation of $y$ can be written as a sum of our underlying signal perturbed by the noise $(\ep_{t_i})$,
\[
y_{t_i} = f_{t_i} + \ep_{t_i}.
\]

We can also express this relationship in a Bayesian framework.  In particular, the distribution of $(y_{t_i})$ is given by
\begin{gather*}
y \sim \mcN(f, \sigma^2 I) \\
f \sim \mcN(\mu, \sigma^2 K)
\end{gather*}
For the moment we assume that $\mu = 0$ and the covariance matrix has a specified form.  We want to derive a Bayesian estimate for $\sigma^2$.  For analytical convenience we will use and Inverse Gamma prior for $\sigma^2$,
\[
\sigma^2 \sim \mathcal{IG}(a/2,b/2);
\]
however, there are perhaps other justifications I am unaware of as well.  

Our program is as follows.  Pick a $\sigma^2$ and generate synthetic data.  Using this synthetic data and Gibbs sampling numerically compute an approximation of the posterior distribution of $\sigma^2$.  As mentioned earlier, the inverse gamma distribuion is nice because there are closed form solutions to the posterior distribution of $\sigma^2$.  We derive these closed form solutions now so that we may check the posterior distribution generated by Gibbs sampling.

\subsection{Analysis}

By our choice of inverse gamma prior this problem has a closed form solution.  We derive that here, which will provide a check on our computational methods.  To remind the reader, the joint distribution of the observed data, the underlying data, and $\sigma^2$ is summarized by
\begin{gather*}
y \sim \mcN(f, \sigma^2 I) \\
f \sim \mcN(0, \sigma^2 K) \\
\sigma^2 \sim \mathcal{IG}(a/2,b/2).
\end{gather*}
We observe $y_{t_i}$, which I will hence forth call $y_i$.  I also am adopting the convention that letters may represent random variables or scalars depending on the context.  

Since we do not observe $f$, we need to marginalize so that we have a joint density for $y$ given $\sigma^2$.  In particular, the model above implies that
\[
p(y | \sigma^2) = \int_{\R^n} p(y | f, \sigma^2) p(f | \sigma^2) df.
\]
Writing this out explicitly we have that
\[
p(y | \sigma^2) \propto \int_{\R^n} \exp \Big\{ \frac{-1}{2 \sigma^2} \Big[ (y-f)^T(y-f) + f^T K^{-1} f \Big] \Big\} df.
\]

There are multiple ways that we could go about making this calculation.  We could approach this problem using a ``Random Variable Approach'' or we could approach this problem by averaging over $f$ as the integral above suggests.  The random variable approach uses properties of normal random variables, such as the fact that normal random variables form a closed vector space.  These properties ultimately depend upon the structure of the density of multivariate normal random variable and the decomposition of positive definite symmetric matrices in particular.  Thus the approaches are not completely independent from one another.

We explicitly calculate the above integral first since this is the more technical procedure.  Pursuing a slightly generalization, 
$p(y|f) = \mcN(f, A)$ and $p(f) = \mcN(0, B)$, we find that
\[
p(y | A, B) \propto \int_{\R^n} \exp \Big\{ \frac{-1}{2} \Big[ (y-z)^T A^{-1}(y-z) + z^T B^{-1} z \Big] \Big\} dz.
\]
Focusing on the term in the exponent we expand and collect terms of $z$ to get
\[
z^T \underbrace{(A^{-1} + B^{-1})}_{C^{-1}} z - 2 z^T A^{-1} y + y^T A^{-1} y
\]
from which we complete the square to produce
\[
(z - CA^{-1}y)^T C^{-1} (z - CA^{-1} y) + y^T(A^{-1} - A^{-1} C A^{-1}) y.
\]
In light of this expression, when we integrate the kernel over $z \in \R^n$ we get
\[
p(y | A, B) \propto \exp \Big\{ \frac{-1}{2} y^T V^{-1} y \Big\}
\]
where
\[
V^{-1} = A^{-1} - A^{-1} C A^{-1}.
\]
Thus $y$ is normally distributed given this model.

Instead of fooling with the density explicitly we can exploit our knowledge of normal random variables.  Notice that we can summarize the system as $Y|F \sim \mcN(F, A)$ and $F \sim \mcN(0, B)$.  Explicitly constructing these random variables we have
\begin{gather*}
Y = F + \nu \; \textmd{ and } \; F = \eta \\
\textmd{ where } \nu \sim \mcN(0,A) \perp \eta \sim \mcN(0,B).
\end{gather*}
This is a valid construction since $\bbE[Y|F] = F$, $\Var[Y|F] = A$, and hence the conditional distribution of $Y$ given $F = f$ must be normal with mean $f$ and variance $A$.  We can then calculate the expectation and variance of $Y$ as
\[
\bbE[Y] = 0 \; \textmd{ and } \Var[Y] = \Var[\nu + \eta] = A + B.
\]
We make use of the fact that the sum of two normal random variables is still normal, a property that is easy to take for granted and is not possesed by most random variables.  If this were not the case then we could not construct the random variables $Y$ and $F$ so easily.  I leave it to the reader to check that $V = A + B$.

Returning to the specific problem at hand we see that $Y | \sigma^2$ is normally distributed as $\mcN(0, \sigma^2 V )$ where $V = I + K$.  We chose an inverse gamma prior for the distribution of $\sigma^2$ for analytic tractablility.  Equivalently we can define $\phi = \sigma^{-2}$ and then let $\phi$ have a Gamma distribution.  This is the route we choose to take presently.  Recall that a gamma distribution is parameterized by shape and scale and has density given by
\[
G[a, b] \sim \phi^{a} e^{-\phi b}.
\]
Our calculations will work out nicely if we take the prior on $\phi$ to be $G[n/2, d/2]$.  Given this prior, the posterior distribution for $\phi$ is
\begin{align*}
p(\phi | y) & \propto \underbrace{\frac{\phi^{p/2}}{|V|^{p/2}} \, e^{-\frac{\phi}{2} y^T V^{-1} y}}_{\textmd{normal}} \; \underbrace{\phi^{n/2-1} e^{-\phi d/2}}_{\textmd{gamma}} \\
& \propto \phi^{\frac{n+p}{2} - 1} \exp \Big\{ - \phi \frac{1}{2}\Big[ d + y^T V^{-1} y \Big] \Big\} \\
& \sim G[n^*/2, d^*/2] \textmd{ where } n^* = n + p \textmd{ and } d^* = d + y^T V^{-1} y.
\end{align*}
Thus we have chosen a conjugate prior.  This analytic solution will provide a check upon our numerical work.

Lastly, before we get to the numerics, we want to examine how our inference changes when we have multiple independent observations.  Independent observations are convinient since they provide for ``on-line'' inference, which means that we can update our knowledge of the unknown parameter when a new observation arrives.

For example, suppose that we have two conditionally independent observations $y_i | \theta$ that are identically distributed.  We can inductively generating a sequence of posterior densities $p(\theta | y_1)$, $p(\theta| y_2, y_1)$ using the independence of $y_1 | \theta$ and $y_2 | \theta$.  In particular, Bayes theorem tells us that
\[
p(\theta | y_1, y_2) \propto p(y_1, y_2 | \theta) \, p(\theta).
\]
Invoking that $y_1 | \theta \perp y_2 | \theta$ we get
\[
p(y_1, y_2 | \theta) = p(y_1 | \theta) \, p(y_2 | \theta).
\]
Thus we can express the posterior distribution for $\theta$ given the observations $y_1$ and $y_2$ as
\[
\underbrace{p(\theta | y_1, y_2)}_{\textmd{posterior given $y_1$, $y_2$}}
\propto p(y_2 | \theta) 
\underbrace{p(y_1 | \theta) p(\theta)}_{\textmd{posterior given $y_1$}}.
\]
We can think of this in the following way.  Observe $y_1$ and generate a posterior distribution for $\theta | y_1$.  This posterior distribution becomes our new ``prior'' distribution for $\theta$.  We are given a new observation $y_2$ and apply Bayes theorem to get the ``posterior'' distribution,
\[
p(\theta | y_1, y_2) \propto p(y_2 | \theta) \, p(\theta | y_1).
\]
In this way, we can always inductively generate posterior distributions by
\[
p(\theta | \, \textmd{old, new}) \propto p(\textmd{new} | \theta) \, p(\theta | \textmd{old})
\]
where ``new'' is the new data and ``old'' is the old data.  The requirement that we make independent draws, given the parameter $\theta$ is essential.  Without that assumption we cannot update our ``posterior'' probability for $\theta$ so easily.

Now let us apply this procedure to the example at hand.  Suppose we have $k$ independent random draws $y_i | \phi$ and that that we are using the prior probability $\phi \sim G[n/2, d/2]$.  The posterior probabilty, given all $k$ observations is then
\[
p(\phi | y_1, \ldots, y_k) = G[n^*/2, d^*/2] \textmd{ where } n^* = n+ pk \textmd{ and } d^* = d + \sum_{i=1}^k
  y_i V^{-1} y_i.
\]
Notice that the observations only enter the equation through the rate parameter of the gamma distribution.  If the $y_i$ are actually sampled as $y_i | \theta^*$, for a common fixed $\theta^*$, then asymptotically our posterior distribution converges to $\theta^*$.

\subsection{Numerical Solutions}

Instead of using the closed form solution to our problem, we want to generate a numerical solution.  This may seem like a superflous thing to do, but we want to examine various ways one can go about constructing numerical solutions.

\subsubsection{Simple Sampling - The Law of Large Numbers}

We chose our likelihoods and priors in this toy model so that a closed form solution existed.  However, sometimes we might decide to model our likelihood or prior so that a closed form solution does not exist.  In such a situation one would like to numerically compute the prior distribution.  We examine the simplest possible case first.

Heuristically, we want to simulate a density.  This is done by constructing a histogram.  Suppose for instance that we want to approximate the density $p(x)$ of the random variable $X$.  We know by the Lebesgue differntiation theorem that
\[
p(x) \sim \frac{1}{|Q_\delta(x)|} \int_{Q_\delta(x)} p(z) dz
\]
for almost every $x$.  We chose to use cubes here instead of balls because this translates naturally to a histogram.  In reality, we could use any set that shrinks nicely.  Rewriting this integral as an expectation we have
\[
p(x) \sim \frac{1}{|Q_\delta(x)|} \bbE[ \1_{Q_\delta(x)}(X) ].
\]
We may approxmiate this expectation with the Law of Large Numbers which roughly says that given any function $f$ we can asymptotically approximate $\bbE[f(X)]$ by
\[
\frac{1}{N} \sum_{i=1}^N f(X_i)
\]
where $X_i$ are independent and have the same distribution as $X$.  Thus we can conclude that
\[
p(x) \sim \frac{1}{|Q_\delta(x)|} \frac{1}{N} \sum_{i=1}^N \1_{Q_\delta(x)} (X_i).
\]
The function $\1_{Q_\delta(x)}(X_i)$ is one if $X_i$ is in our designated cube and zero if it is not.  Thus the above summation counts the number of times the random variable $X_i$ is in said cube out of $N$ draws.

If we think about partitioning our space by equispaced cubes at the points $\{x_j\}$ then we can construct a histogram by
\[
h(x_j) = \sum_{i=1}^N \1_{Q_\delta(x_j)}(X_i).
\]
Notice that, since we chose cubes of identical size, the ``area'' under the integral is simply the number of samples multiplied by the ``area'' of each bin, $N | Q_\delta(x_j) |$.  Thus the normalized histogram is
\[
h^*(x_j) = \frac{1}{|Q_\delta(x)|}\frac{1}{N} \sum_{i=1}^N \1_{Q_\delta(x_j)}(X_i).
\]
This is identical to our approximation of the density $p(x_j)$.  Hence the normalized histogram approximates the density of the random variable $X$ at the points $x_j$.  This is an asymptotic statement and we need to $N$ to be large to have a good approximation.

Now let us return to an application of this simple sampling to Bayesian statistics.  Suppose that you know how to evaluate the conditional density $p(y|\phi)$ but you do not know how to evaluate the prior density $p(\phi)$.  You want to calculate the posterior probability $p(\phi | y) \propto p(y | \phi) p(\phi)$.  Think of the conditional probability as a function of $\phi$ here.  The data $y$ has been fixed.  We can approximate $p(\phi)$ proportionally with a histogram, as seen in our work above.  Thus
\[
p(\phi_j | y) \propto p(y|\phi_j) \sum_{i=1}^N \1_{Q_\delta(\phi_j)}(S_j)
\]
where $S_j$ has density given by $p(\phi)$.  One can then empirically normalize this approximation to get the density $p(\phi_j | y)$.

Suppose further that one is given a sequence of conditionally independent observations $y_i | \phi$.  Then the joint conditional density of all the $y_i$ given $\phi$ is
\[
p(y_1, \ldots, y_n | \phi) = p(y_1 | \phi) \cdots p(y_n | \phi).
\]
Hence we can approximate the posterior distribution for $\phi$ by
\[
p(\phi | y_1, \ldots, y_n) \propto p(y_1 | \phi) \cdots p(y_n | \phi) p(\phi)
\]
which we can be approximated as above using a histogram as
\[
p(\phi_j | y_1, \ldots, y_n) \propto p(y_1 | \phi_j) \cdots p(y_n | \phi_j) \sum_{i=1}^N \1_{Q_\delta(\phi_j)}(S_j).
\]

\end{document}
